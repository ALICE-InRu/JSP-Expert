% TEXINPUTS=.:/home/hei2/Documents/LaTeX/myTexStyles:
% created for Journal of Heuristics 
% Time-stamp: "2011-07-18 14:17 hei2"
% if all fonts computer modern use -G1
% dvips -Ppdf -G0 <filename>
% http://www.springer.de/comp/lncs/authors.html

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

%! xelatex joh.tex

\RequirePackage{fix-cm} 

\documentclass[smallextended]{svjour3} 
\smartqed  % flush right qed marks, e.g., at end of proof

\makeatletter
%\def\cl@chapter{\cl@chapter \@elt {theorem}}%bug in class
\def\cl@chapter{\@elt {theorem}}
\makeatother

\usepackage{mathptmx} % use Times fonts if available on your TeX system

\journalname{Journal of Heuristics}

\title{Learning Linear Composite Dispatch Rules for Scheduling}
\subtitle{Case study for the job-shop problem}

\author{Helga Ingimundardottir \and Thomas Philip Runarsson }
%\authorrunning{Short form of author list} % if too long for running head

\institute{H. Ingimundardottir \at
	Dunhaga 5, IS-107 Reykjavik, Iceland \\
	Tel.: +354-525-4704\\
	Fax: +354-525-4632\\
	\email{hei2@hi.is}\\
	\and
	T.P. Runarsson \at
	Hjardarhagi 2-6, IS-107 Reykjavik, Iceland \\
	Tel.: +354-525-4733\\
	Fax: +354-525-4632\\
	\email{tpr@hi.is}\\
}
\date{Received: \today / Accepted: date}
% The correct dates will be entered by the editor

\input{shorthand} % put your own shorthand declarations in this document

\begin{document}
\maketitle

\selectlanguage{english}

\begin{abstract}
Instead of creating new \dr s in an ad-hoc manner,
this study gives a framework on how to analyse heuristics for scheduling 
problems.  Before starting to create new \cdr s, 
meticulous research on optimal schedules can give an abundance of valuable 
information that can be utilised for learning new models.  For instance, it's 
possible to seek out when the scheduling process is most susceptible to 
failure.  Furthermore, the stepwise optimality of individual features imply 
their explanatory predictability. From which, a preference set is collected and 
a preference based learning model is created based on what feature states are 
preferable to others w.r.t. the end result, here minimising the final makespan.
Moreover, the learned model should be created in an active  imitation learning 
manner in order for learned state spaces to properly represent one that should 
be eventually encountered.
By doing so it's possible to learn new \cdr s that 
outperform the models they are based on. 
Even though this study is based around the \jsp\ scheduling problem, it can be 
generalised to any kind of sequential combinatorial problem.
\keywords{Scheduling \and Composite \dr s \and Machine Learning 
\and Feature Selection}
\end{abstract}

\section{Introduction}\label{sec:introduction}
The problem is to learn new problem specific priority \dr s for scheduling. 
A subclass of scheduling problems is the \jsp\ scheduling problem (JSP), 
which is widely studied in operations research.  \JSP\ deals with the 
allocation of tasks of competing resources where its goal is to optimise a 
single or multiple objectives.  Its analogy is from manufacturing industry 
where a set of jobs are broken down into tasks that must be processed on 
several machines in a workshop.  
Furthermore, its formulation can be applied on a wide variety of practical 
problems in real-life applications which involve decision making, therefore its
problem-solving capabilities has a high impact on many manufacturing 
organisations.

Generally, \jsp\ is solved by applying a hand-crafted dispatching rule (DR) for 
a given problem space. 
Due to the exorbitant amounts of DRs to choose from, and any kind of alteration 
to the problem space, this can become quite a time-consuming selection process 
for the heuristic designer, which any kind of automation would alleviate 
immensely. 
% automation for selecting DRs #1 - Portfolio
With meta heuristics one can use existing DRs, and use for example 
portfolio-based algorithm selection \cite{Rice76,Gomes01}, either based on a 
single instance or class of instances \cite{Xu07} to determine which DR to 
choose from. 
% automation for selecting DRs #2 - ACO
Implementing ant colony optimisation to select the best DR 
from a selection of nine DRs for \JSP, experiments from \cite{Korytkowski13} 
showed that the choice of DR do affect the results and that for all performance 
measures considered. They showed that it was better to have a all the DRs to 
choose from rather than just a single DR at a time.

Heuristics algorithms for scheduling are typically either a construction or 
improvement heuristics. The improvement heuristic starts with a complete 
schedule and then tries to find similar, but better schedules.  A construction 
heuristic starts with an empty schedule and adds one job at a time until the 
schedule is complete. The work presented here will focus on construction 
heuristics, \dr s, although the techniques developed could be adapted to 
improvement heuristics also. 

Genetic algorithms (GA) are one of the most widely used approaches in \JSP\ 
literature \cite{Pinedo08}. However, in that case an extensive number of 
schedules need to be evaluated, and even for low dimensional \JSP\ that can 
quickly become computationally infeasible.
GAs can be used directly on schedules 
\cite{Cheng96,Cheng99,Tsai07,Qing-dao-er-ji12,Ak12,Meeran12}, however, in that 
case there are many concerns that need to be dealt with. To begin with there 
are nine encoding schemes for representing the schedules \cite{Cheng96}. 
In addition, there has to be special care taken when applying cross-over and 
mutation operators in order for the schedules, now in the role of 
`chromosomes,' to still remain feasible. 

Another approach is to apply GAs indirectly to \JSP , via dispatching rules, 
i.e., Dispatching Rules Based Genetic Algorithms (DRGA) 
\cite{Vazquez-Rodriguez09,Dhingra10,Nguyen13} where a solution is no longer a 
\emph{proper} schedule but a \emph{representation} of a schedule via applying 
certain dispatching rules consecutively. 
DRGA are a special case of \emph{genetic programming} (GP) \cite{Koza05} which 
is 
the most predominant approach in hyper-heuristics is a framework of creating 
\emph{new} heuristics from a set of  predefined heuristics via GA optimisation 
\cite{Burke10}. 

A prevalent approach to solving \JSP\ is to combine several relatively \sdr s 
such that they may benefit each other for a given problem space. 
% automation for selecting DRs #3 - GA
The approach in \cite{InRu14}, was to automate that selection, by 
translating dispatching rules into measurable features and optimising what 
their contribution should be via evolutionary search. The framework is straight 
forward and easy to implement and showed promising and robust results, 
as models were trained on a lower dimension, and validated on higher dimension. 
Moreover, \cite{InRu14} showed that the choice of objective 
function  for evolutionary search is worth investigating. Since the 
optimisation is based on minimising the expected mean of the fitness function 
over a large set of problem instances, which can vary within. Then normalising 
the objective function can stabilise the optimisation process away from local 
minima. 

% automation for selecting DRs #4 - GA
By applying GP on a terminal set of job-attributes for flexible \jsp,\footnote{
    Flexible \jsp\ allows jobs to be processed on different machines, i.e., the 
    problem has the additional complexity of making a routing decision of 
    operations to machines.} 
\cite{Tay08} optimised a multi-objective \jsp~(transformed into a single 
objective by linearly combining their objectives) with promising results 
compared to the benchmarks DRs from the literature.
The main drawback, is that the rules from their GP framework is quite complex, 
and difficult to contrive a meaningful description to a layman.
In fact, \cite{Hildebrandt2010} revisited the experiments from \cite{Tay08} for 
dynamic \jsp,\footnote{\Jsp\ is considered dynamic when the processing times 
are not known prior to their arrival.} and tested it against some \sdr s, and 
found that it only slightly outperformed ERD-rule, and was beat by SPT-rule. 
The reason behind this staggering change in performance, is most likely due to 
the choice of objective function, and the underlying problem spaces that were 
used in training. 
It's argued that the randomly generated problem instances aren't a 
proper representative for real-world long-term \jsp\ applications, e.g., by the 
narrow choice of release times, yielding schedules that are overloading in the 
beginning phases.

A novel iterative \dr s that were evolved with GP for \JSP, \cite{Nguyen13} 
learned from completed schedules in order to iteratively improve new ones. 
At each dispatching step, the method can utilise the current feature space to 
\emph{correctify} some possible \emph{bad} dispatch made previously (sort of 
reverse lookahead). Their method is straightforward, and thus easy to 
implement and more importantly computationally inexpensive, although the 
authors do stress that there is still remains room for improvement.

Adopting a two-stage hyper-heuristic approach to generate a set of 
machine-specific DRs for dynamic \jsp, \cite{Pickardt2013} used GP to evolve 
\cdr s (CDR) from basic attributes, along with evolutionary algorithm to assign 
a CDR to a specific machine. 
The problem space consists of \jsp s in semiconductor manufacturing, with 
additional shop constraints, as machines are grouped to similar work centres, 
which can have different set-up time, workload, etc. 
In fact, the GP emphasised on efficiently dispatching on the work 
centres with set-up requirements and batching capabilities, which are rules 
that are non-trivial to determine manually.

In the field of Artificial Intelligence, \cite{Meeran12} point out that despite 
their `intelligent' solutions, the effectiveness of finding the optimum has 
been rather limited. This is the general case for GAs, as they are not well 
suited for fine-tuning around the optimum \cite{Cheng99}. 
However, combined with local-search methodologies, they can be improved upon 
significantly, as \cite{Meeran12} showed with the use of a hybrid method using 
Genetic Algorithms (GA) and Tabu Search (TS). 
Therefore, getting the best of both worlds, namely, the diverse global search 
obtained from GA and being complemented with the intensified local search 
capabilities of TS. 
Now, hybridisation of global and local methodologies is non-trivial. In 
general combination of the two improves performance, however, they often come 
at a great computational cost.  

Using improvement heuristics, \cite{Zhang95} studied space shuttle payload 
processing by using reinforcement learning (RL), in particular, temporal 
difference learning. 
Starting with a relaxed problem, each job was scheduled as early as its 
temporal partial order would permit, there by initially ignoring any resource 
constraints on the machines, yielding the schedule's critical path. Then the 
schedule would be repaired so the resource constraints were satisfied in the 
minimum amount of iterations.

Meta learning can be very fruitful in RL, as experiments from 
\cite{Kalyanakrishnan11} discovered some key discriminants between 
competing algorithms for their particular problem instances, which provided 
them with a hybrid algorithm which combines the strengths of the algorithms.

Using case based reasoning for timetable scheduling, training data in 
\cite{Burke06} is guided by the two best heuristics in the literature.
They point out that in order for their framework to be successful, problem 
features need to be sufficiently explanatory and training data need to be 
selected carefully so they can suggest the appropriate solution for a specific 
range of new cases. Stressing the importance of meaningful feature selection. 

A recent editorial of the state-of-the-art approaches in advanced \dr s for 
large-scale manufacturing systems by \cite{Chen13} points out that:
\lq\lq ... most traditional \dr s are based on historical data. 
With the emergence of data mining and on-line analytic processing, dispatching 
rules can now take predictive information into account.\rq\rq~The importance of 
automated discovery of DR was also emphasised by \cite{Monch13}. 
Several of successful implementations in the field of semiconductor wafer 
fabrication facilities are discussed, however, this sort of investigation is 
still in its infancy.

An alternative to hand-crafting heuristics, is to implement an automatic way 
of learning heuristics using a data driven approach. 
Data can be generated using a known heuristic, such an approach is taken in 
\cite{Siggi05} for single-machine \jsp\ where a LPT-rule is applied.
Afterwards, a decision tree is used to create a \dr\ with similar logic. 
However, this method cannot outperform the original LPT-rule used to guide the 
search. 
This drawback is confronted in \cite{Malik08,Russell09,Siggi10} by using an 
optimal scheduler, computed off-line. 
The optimal solutions are used as training data and a decision tree 
learning algorithm applied as before. Preferring simple to complex models, the 
resulting \dr s gave significantly better schedules than using popular 
heuristics in that field, and a lower worst-case factor from optimality. 

Although, using expert policy for creating training data gives vital 
information on how to learn good scheduling rules, it is a good starting point, 
but not sufficient. This is due to the fact our models are only based on 
optimal decisions, then once we make a suboptimal choice we are in uncharted 
territory and its effects are relatively unknown. For this reason, it is of 
paramount importance to inspect the actual end-performance when choosing a 
suitable model, not just staring blindly at the classification accuracy. 
When it comes to designing algorithms there needs to be emphasis on where to 
innovate and imitate when visiting state-spaces. 
This study will show, that when using these guidelines when accumulating 
training data for supervised  learning, it's possible to automate its 
generation in such a way that the resulting model will be an accurate 
representative of the instances it will later come across. 
For this purpose, \JSP\ is used as a case study to illustrate a methodology for 
generating meaningful training set autonomously, which can be successfully 
learned using preference-based imitation learning (IL).

The focus on this study is better understanding of \emph{how} and \emph{when} 
\dr s work in general, in order to mediate the set-up for the 
learning problem.
For this reason, we propose a framework for learning the indicators of optimal 
solutions, such as done by \cite{Siggi10}, as an in-depth analysis of a expert 
policy gives a benchmark of what is theoretically possible to learn. 
The study shows that during the scheduling process, it varies \emph{when} it's 
most fruitful to make the `right' decision, and depending on the problem space 
those pivotal moments can vary greatly. 

The outline of the paper is the following, \cref{sec:problemdef} gives the 
mathematical formalities of the scheduling problem, and  
\cref{sec:DR} describes the main attributes for \jsp, 
and goes into how to create schedules with \dr s. 
\Cref{sec:learnOPT} sets up the framework for learning from optimal schedules. 
In particular, the probability of choosing optimal decisions and the effects of 
making a suboptimal decision. Furthermore, the optimality of common \sdr s is 
investigated.
With those guidelines, \cref{ch:expr:CDR} goes into detail how to create 
meaningful \cdr s using preference learning, focusing on how to 
compare operations and collect training data with the importance of good state 
sampling. 
\Cref{sec:il:active,sec:il:passive} explain the trajectories for 
sampling meaningful schedule state-spaces used in preference learning, either 
using passive or active imitation learning. 
Experimental results are jointly presented in \cref{sec:il:expr} with 
comparison for a single randomly generated problem space. Furthermore, some 
general adjustments for performance boost is also considered.
The paper finally concludes in \cref{sec:con} with discussion and conclusions.

%\setcounter{tocdepth}{2} \tableofcontents

\section{\Jsp~Scheduling}\label{sec:problemdef}
The \jsp~problem (\JSP) involves the scheduling of jobs on a set of 
machines. Each job consists of a number of operations which are then processed 
on the machines in a predetermined order. An optimal solution to the problem 
will depend on the specific objective. 
% For example, the optimal schedule may be the time needed to complete
% all jobs, i.e., the minimum makespan.

In this study we will consider the $n\times m$ \JSP, where $n$ jobs, 
$\mathcal{J}=\{J_j\}_{j=1}^n$, are scheduled on a finite set, 
$\mathcal{M}=\{M_a\}_{a=1}^m$, of $m$ machines. The index $j$ refers to a job 
$J_j\in\mathcal{J}$ while the index  $a$ refers to a machine 
$M_a\in\mathcal{M}$. 
If a job requires a number of processing steps or operations, then the pair 
$(j,a)$ refers to the operation, i.e., processing the task of job $J_j$ on 
machine $M_a$. 

Each job $J_j$ has an indivisible operation time (or cost) on machine $M_a$, 
$p_{ja}$, which is assumed to be integral and finite. 
Starting time of job $J_j$ on machine $M_a$ is denoted $x_s(j,a)$ and its 
end time is denoted $x_e(j,a)$ where, 
\begin{equation}  x_e(j,a):=x_s(j,a)+p_{ja} \end{equation} 
Each job $J_j$ has a specified processing order through the machines, it is a 
permutation vector, $\vsigma_j$, of $\{1,..,m\}$, representing a job $J_j$ can 
be processed on $M_{\vsigma_j(a)}$ only after it has been completely processed 
on $M_{\vsigma_j(a-1)}$, i.e.,
\begin{equation}\label{eq:permutation}
x_s(j,\vsigma_j(a)) \geq x_e(j,\vsigma_j(a-1)) 
\end{equation}
for all $J_j\in\mathcal{J}$ and $a\in\{2,..,m\}$. 
Note, that each job can have its own distinctive flow pattern through the 
machines, which is independent of the other jobs. 
However, in the case that all jobs share the same \emph{fixed} permutation 
route, referred to as \fsp~(\FSP). 
A commonly used subclass of \FSP\ in the literature is permutation \fsp, which 
has the added constraint that the processing order of the jobs on the machines 
must be identical as well, i.e., no passing of jobs allowed \cite{Stafford88}.

The disjunctive condition that each machine can handle at most one job at a 
time is the following,
\begin{equation}\label{eq:oneJobPerMac}
x_s(j,a) \geq x_e(j',a) \quad\textrm{or}\quad x_s(j',a) \geq x_e(j,a)  
\end{equation}
for all $J_j,J_{j'}\in\mathcal{J},\; J_j\neq J_{j'}$ and $M_a\in\mathcal{M}$. 

The objective function is to minimise its maximum completion times for all 
tasks, commonly referred to as the makespan, $C_{\max}$, which is defined as 
follows,
\begin{equation}
C_{\max} := 
\max\left\{x_e(j,\vsigma_j(m))\;\middle|\;J_j\in\mathcal{J}\right\}.\label{eq:makespan}
\end{equation} 
This family of scheduling problems is denoted by $J||C_{\max}$ 
\cite{Pinedo08}.
Additional constraints commonly considered are job release-dates and due-dates 
or sequence dependent set-up times, however, these will not be considered here. 

In order to find an optimal (or near optimal) solution for scheduling problems 
one could either use exact methods or heuristics methods. Exact methods 
guarantee an optimal solution. However, \jsp\ scheduling is strongly NP-hard 
\cite{Garey76:NPhard}. Any exact algorithm generally suffers from the curse of 
dimensionality, which impedes the application in finding the global optimum in 
a reasonable amount of time. 
Using a state-of-the-art software for solving scheduling problems, such as 
LiSA %\footnote{LiSA is distributed under the GNU General Public License, and 
%is available at \url{http://www.math.ovgu.de/Lisa.html}.} 
(A Library of Scheduling Algorithms) \cite{LiSA}, which includes a specialised 
version of branch and bound that manages to find optimums for \jsp\ problems of 
up to $14\times14$ \cite{Ru12}. However, problems that are of greater size, 
become intractable. 
Heuristics are generally more time efficient but 
do not necessarily attain the global optimum. Therefore, \jsp\ has the 
reputation of being notoriously difficult to solve. 
As a result, it's been widely studied in deterministic scheduling theory and 
its class of problems has been tested on a plethora of different solution 
methodologies from various research fields \cite{Meeran12}, all from simple and 
straight forward \dr s to highly sophisticated frameworks.


\section{Priority Dispatching Rules} \label{sec:DR}

Priority \dr s determine, from a list of incomplete jobs, 
$\mathcal{L}$, which job should be dispatched next. This process is illustrated 
in \Cref{fig:jssp:example}, where an example of 
a temporal partial schedule of six-jobs scheduled on five-machines is illustrated. 
The numbers in the boxes represent the job identification $j$. 
The width of the box illustrates the processing times for a given job for a 
particular machine $M_a$ (on the vertical axis). 
The dashed boxes represent the resulting partial schedule for when a particular 
job is scheduled next. 
Moreover, the current $C_{\max}$ is denoted by a dotted vertical line. 
The object is to keep this value as small as possible once all operations are 
complete. As shown in the example there are $15$ operations already scheduled. 
The \textit{sequence} of dispatches used to create this partial schedule is,
\begin{equation}
	\vchi=\left(J_3,J_3,J_3,J_3,J_4,J_4,J_5,J_1,J_1,J_2,J_4,J_6,J_4,J_5,J_3\right)
\end{equation}
and refers to the sequential ordering of job dispatches to machines, i.e., $(j,a)$; 
the collective set of allocated jobs to machines is interpreted by its 
sequence, is referred to as a \emph{schedule}.
A \emph{scheduling policy} will pertain to the manner in which 
the sequence is determined from the available jobs to be scheduled. 
In our example the available jobs is given by the job-list
$\mathcal{L}^{(k)}=\{J_1,J_2,J_4,J_5,J_6\}$ with the five potential jobs 
to be dispatched at step $k=16$ (note that $J_3$ is completed).

Deciding which job to dispatch is, however, not sufficient, one must also know 
where to place it. In order to build tight schedules it is sensible to place a 
job as soon as it becomes available and such that the machine idle time is 
minimal, i.e., schedules are non-delay. 
There may also be a number of different options for such a placement. 
In \cref{fig:jssp:example} one observes that $J_2$, to be scheduled on $M_3$, 
could be placed immediately in a slot between $J_3$ and $J_4$, or after $J_4$ 
on this machine. 
If $J_6$ had been placed earlier, a slot would have been created between it and 
$J_4$, thus creating a third alternative, namely scheduling $J_2$ after $J_6$. 
The time in which machine $M_a$ is idle between consecutive jobs $J_j$ and 
$J_{j'}$ is called idle time, or slack, 
\begin{equation} 
s(a,j):=x_s(j,a)-x_e(j',a) \label{eq:slack}
\end{equation}
where $J_j$ is the immediate successor of $J_{j'}$ on $M_a$. 

Construction heuristics are designed in such a way that it limits the search 
space in a logical manner, respecting not to exclude the optimum. Here, the 
construction heuristic, $\Upsilon$, is to schedule the dispatches as closely 
together as possible, i.e., minimise the schedule's idle time. 
More specifically, once an operation $(j,a)$ has been chosen from the job-list, 
$\mathcal{L}$, by some dispatching rule, it can placed immediately after (but 
not prior) $x_e(j,\vsigma_j(a-1))$ on machine $M_a$ due to constraint 
\cref{eq:permutation}. 
However, to guarantee that constraint \cref{eq:oneJobPerMac} is not violated, 
idle times $M_a$ are inspected, as they create flow time  which $J_j$ can 
occupy. Bearing in mind that $J_j$ release time is $x_e(j,\vsigma_j(a-1))$ one 
cannot implement \cref{eq:slack} directly, instead it has to be updated as 
follows,
\begin{equation}
\tilde{s}(a,j'):= x_s(j'',a)-\max\{x_e(j',a),x_e(j,\vsigma_j(a-1))\} % 
%\textrm{ if } x_e(j,\vsigma_j(a-1)\geq x_e(j',a) 
\end{equation}
for all already dispatched jobs $J_{j'},J_{j''}\in \mathcal{J}_a$ where 
$J_{j''}$ is $J_{j'}$ successor on $M_a$. Since pre-emption is not allowed, the 
only applicable slots are whose idle time can process the entire operation, 
i.e.,
\begin{equation}
\tilde{S}_{ja} := \condset{J_{j'}\in \mathcal{J}_a}{\tilde{s}(a,j')\geq p_{ja} 
}\label{eq:slots}.
\end{equation} 
The placement rule applied will decide where to place the job and 
is an intrinsic heuristic of the construction heuristic, chosen independently 
of the priority dispatching rule applied. 
Different placement rules could be considered for selecting a slot from 
\cref{eq:slots}, e.g., if the main concern were to utilise the slot space, then 
choosing the slot with the smallest idle time would yield a closer-fitted 
schedule and leaving greater idle times undiminished for subsequent dispatches 
on $M_a$.
In our experiments cases were discovered where such a placement can rule out 
the possibility of constructing optimal solutions.
This problem, however, did not occur when jobs are simply placed as early as 
possible, which is beneficial for subsequent dispatches for $J_j$. 
For this reason it will be the placement rule applied here.

\begin{figure}[t!]\centering
	\includegraphics[width=0.8\textwidth]{figures/jssp_example}
	\caption[Gantt chart of a partial \JSP\ schedule]{Gantt chart of a
		partial \JSP\ schedule after 15 dispatches: Solid and dashed boxes
		represent $\vchi$ and $\mathcal{L}^{(16)}$, respectively. Current
		$C_{\max}$ denoted as dotted line.}
	\label{fig:jssp:example}
\end{figure}

%\subsection{Priority Dispatching Rules}\label{ch:dispatchrules}
% \subsection{Simple priority \dr s}
%Dispatching rules are of a construction heuristics, where one starts with an 
%empty schedule and adds sequentially on %one operation (or tasks) at a time. 

Priority \dr s will use attributes of operations, such as processing time, 
in order to determine the job with the highest priority. 
Consider again \Cref{fig:jssp:example}, if the job with the shortest processing 
time (SPT) were to be scheduled next, then $J_2$ would be dispatched. 
Similarly, for the longest processing time (LPT) heuristic, $J_5$ would have the highest priority. 
Dispatching can also be based on attributes related to the partial schedule. 
Examples of these are dispatching the job with the most work remaining (MWR) or 
alternatively the least work remaining (LWR). A survey of more than $100$ of 
such rules are presented in \cite{Panwalkar77}. 
However, the reader is referred to an in-depth survey for simple or 
\emph{\sdr} (SDR) by \cite{Haupt89}.  
The SDRs assign an index to each job in the job-list and is generally only 
based on a few attributes and simple mathematical operations.

\begin{table}[t!] \centering
	\caption[Attribute space $\mathcal{A}$ for \JSP]{Attribute space 
	$\mathcal{A}$ for \JSP\ where job $J_j$ on machine $M_a$ given the 
	resulting temporal schedule after operation $(j,a)$.
	}
	\label{tbl:jssp:feat}
	{\setlength{\tabcolsep}{3pt} \input{tables/features-description}}
\end{table}

Designing priority \dr s requires recognizing the important 
attributes of the partial schedules needed to create a reasonable scheduling rule. 
These attributes attempt to grasp key features of the schedule being 
constructed. Which attributes are most important will necessarily depend on the
objectives of the scheduling problem. 
Attributes used in this study applied for 
each possible operation are given in \cref{tbl:jssp:feat}, where the set of 
machines already dispatched for $J_j$ is $\mathcal{M}_j\subset\mathcal{M}$, 
and similarly, $M_a$ has already had the jobs $\mathcal{J}_a\subset\mathcal{J}$ 
previously dispatched.
The attributes of particular interest were obtained by inspecting the 
aforementioned SDRs. Attributes \phiJobRelated\ and \phiMacRelated\ are 
job-related and machine-related, respectively.
In fact, \cite{Pickardt2013} note that in the current literature, there is a 
lack of global perspective in the attribute space, as omitting them won't 
address the possible negative impact an operation $(j,a)$ might have on other 
machines at a later time, it is for that reason we consider attributes such as 
\phiSlackRelated, that are slack related and are a means of indicating the 
current quality of the schedule.
All of the attributes, $\vphi$, vary throughout the scheduling process, 
w.r.t. operation belonging to the same time step $k$, with the exception of 
\phijobTotProcTime\ and \phimacTotProcTime\ which are static for a given 
problem instance but varying for each $J_j$ and $M_a$, respectively. 

Priority \dr s are attractive since they are relatively easy to 
implement, fast and find reasonable schedules. In addition, they are relatively 
easy to interpret, which makes them desirable for the end-user.
However, they can also fail unpredictably. 
A careful combination of \dr s have been shown to perform significantly better 
\cite{Jayamohan04}. These are referred to as \emph{\cdr s} 
(CDR), where the priority ranking is an expression of several \dr s. 
CDRs deal with a greater number of more complicated functions (or features) 
constructed from the schedules attributes. In short, a CDR is a combination of 
several DRs. 
For instance let $\pi$ be a CDR comprised of $d$ DRs, then the index $I$ for 
$J_j\in\mathcal{L}^{(k)}$ using $\pi$ is, 
\begin{equation}	I_j^{\pi} = \sum_{i=1}^d w_i \pi_i(\vchi^j) 
    \label{eq:CDR}
\end{equation}
where $w_i>0$ and $\sum_{i=0}^d w_i = 1$ with $w_i$ giving the weight of the 
influence of $\pi_i$ (which could be a SDR or another CDR) to $\pi$. Note, 
each $\pi_i$ is function of $J_j$'s attributes from the current sequence 
$\vchi$, where $\vchi^j$ implies that $J_j$ was the latest dispatch, i.e., the 
partial schedule given $\chi_k=J_j$.

At each time step $k$, an operation is dispatched which has the highest 
priority.  If there is a tie, some other priority measure is used. Generally 
the \dr s are static during the entire scheduling process, however, ties could 
also be broken randomly (RND). 

Investigating 11 SDRs for JSP, \cite{Lu13} created a pool of 33 CDRs that 
strongly outperformed the ones they were based on, by using multi-contextual 
functions based on either on job waiting time or machine idle time 
(similar to \phiwait\ and \phimacSlack\ in \cref{tbl:jssp:feat}), i.e., the 
CDRs are a combination of those two key attributes and then the SDRs. 
However, there are no combinations of the basic SDRs explored, only those two 
attributes.  
Similarly, using priority rules to combine 12 existing DRs from the literature, 
\cite{Yu13} had 48 CDR combinations, yielding 48 different models 
to implement and test. 
It is intuitive to get a boost in performance by introducing new CDRs, since 
where one DR might be failing, another could be excelling so combining them 
together should yield a better CDR. However, these approaches introduce fairly 
ad-hoc solutions and there is no guarantee the optimal combination of 
\dr s are found.

The \cdr\ presented in \cref{eq:CDR} can be considered as a special case of a 
the following general linear value function,
\begin{equation}\label{eq:jssp:linweights}
	\pi(\vchi^j)=\sum_{i=1}^d w_i \phi_i(\vchi^j).
\end{equation}
when $\pi_i(\cdot)=\phi_i(\cdot)$, i.e., a composite function of the features 
from \cref{tbl:jssp:feat}. Finally, the job to be dispatched, $J_{j^*}$, 
corresponds to the one with the highest value, i.e.,
\begin{equation}\label{eq:jstar}
	J_{j^*}=\argmax_{J_j\in \mathcal{L}}\; \pi(\vchi^j)
\end{equation}
Similarly, \sdr s may be described by this linear model. For instance, let all 
$w_i=0$, but with following exceptions: $w_1=-1$ for SPT, $w_1=+1$ for LPT, 
$w_7=-1$ for LWR and $w_7=+1$ for MWR. Generally the weights $\vec{w}$ are 
chosen by the designer or the 
rule apriori.  A more attractive approach would be to learn these weights from 
problem examples directly. We will now investigate how this may be accomplished.

\section{Performance Analysis of Priority Dispatching Rules}\label{sec:learnOPT}

In order to create successful \dr s, a good starting point is to 
investigate the properties of optimal solutions and hopefully be able to learn 
how to mimic the construction of such solutions. For this, we follow optimal solutions, 
obtained by using a commercial software package \cite{gurobi}, and inspect 
the probability of SDRs being optimal, which serves as an indicator of how hard 
it is to put our objective up as a machine learning problem. 
However, we must also take into consideration the end-goal, which is minimising 
\namerho, because it's its relationship to stepwise optimality is not fully 
understood.


In this \lcnamecref{sec:learnOPT} we will describe concerns that must be 
addressed when learning new priority \dr s. At the same time we will describe 
the experimental set-up used in our study. 
%The matters that must be addressed are as follows
%\begin{enumerate*}[itemjoin*={{, and finally }}]
%\item the problem instances used for learning and their optimal solutions
%\item the reconstruction of the optimal solution using a \dr
%\item the construction of the training set used for learning
%\item the machine learning algorithm applied
%\end{enumerate*}

\subsection{Problem Instances}\label{sec:data:sim}

The class of problem instances used in our studies is the \jsp\ scheduling 
problem described in \cref{sec:problemdef}. Each instance will have 
different processing times, machine ordering and dimensions. Each instance will 
therefore create different challenges for a priority dispatching rule. 
Dispatching rules learned will be customised for the problems used for their 
training. For real world application using historical data would be most 
appropriate. The aim would be to learn a dispatching rule that works well on 
average for a given distribution of problem instances. To illustrate the performance 
difference of priority \dr s on different problem distributions, 
within the same class of problems, consider the following three cases.
Problem instances for JSP are generated stochastically by fixing the number of 
jobs and machines to ten. A discrete processing time is sampled independently 
from a discrete uniform distribution from the interval $I=[u_1,u_2]$, i.e., 
$\vec{p}\sim \mathcal{U}(u_1,u_2)$. 
The machine order is a random permutation of all of the machines in the 
\jsp. Two different processing times distributions were explored, namely 
\jrnd{n}{m} where $I=[1,99]$ and \jrndn{n}{m} where $I=[45,55]$. These 
instances are referred to as random and random-narrow, respectively. In 
addition we consider the case where the machine order is fixed and the same for 
all jobs, i.e. $\vsigma=\{1,\ldots,m\}$ where 
$\vec{p}\sim\mathcal{U}(1,99)$. 
These jobs are denoted by \frnd{n}{m} and is analogous to \jrnd{n}{m}.

The goal is to minimise the makespan, $C_{\max}$. The optimum 
makespan is denoted $C_{\max}^{\pi_\star}$ (using the expert policy 
$\pi_\star$), and the makespan obtained from the 
scheduling policy $\pi$ under inspection by $C_{\max}^{\pi}$. Since the optimal 
makespan varies between problem instances the performance measure is the 
following,
\begin{equation}\label{eq:rho}
\rho=\frac{C_{\max}^{\pi}-C_{\max}^{\pi_\star}}{C_{\max}^{\pi_\star}}\cdot
100\%
\end{equation}
which indicates the percentage relative deviation from optimality. 
Note, \cref{eq:rho} measures the discrepancy between predicted value and true 
outcome, and is commonly referred to as a loss function, which we would like to 
minimise for $\pi$.

\Cref{fig:boxplot:SDR} depicts the box-plot for \cref{eq:rho} when using the 
SDRs from \cref{sec:DR} for all of the problem spaces from \cref{tbl:data:sim}.
These box-plots show the difference in performance of the various SDRs. The MWR performs on average the best on the \jrnd{n}{m} and \jrndn{n}{m} problems instances, whereas for \frnd{n}{m} is is LWR that performs best. It is also interesting to observe that all but the MWR perform statistically worse than random job dispatching on the \jrnd{n}{m} and \jrndn{n}{m} problems instances.

\begin{table}\centering
  \caption[Problem space distributions used in experimental studies.]{Problem 
    space distributions used in experimental studies. Note, problem instances 
    are synthetic and each problem space is i.i.d. 
  }\label{tbl:data:sim}
  \input{tables/data-sim}
\end{table} 

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/{boxplotRho_SDR_10x10}.pdf}
  \caption{Box-plot for deviation from optimality, $\rho$, (\%) for SDRs}
  \label{fig:boxplot:SDR}
\end{figure}

\subsection{Reconstructing optimal solutions}\label{sec:opt:sdr}

When building a complete schedule, $K=n\cdot m$ dispatches must be made 
sequentially.  A job is placed at the earliest available time slot for its next 
machine, whilst still fulfilling that each machine can handle at most one job 
at each time, and jobs need to have finished their previous machines according 
to their machine order. Unfinished jobs, referred to as the job-list denoted 
$\mathcal{L}$, are dispatched one at a time according to a deterministic 
scheduling policy (or heuristic), and its pseudo-code is given  in 
\cref{pseudo:constructJSP}. 
After each dispatch\footnote{Dispatch and time step are used interchangeably.} 
the schedule's current features (cf. \cref{tbl:jssp:feat}) are updated based on 
the half-finished schedule, $\vchi$. 
For each possible post-decision state the temporal features are collected (cf. 
\cref{pseudo:constructJSP:phi}) forming the feature set, $\Phi$, based on all 
$N_{\text{train}}$ problem instances available, namely, 
\begin{equation} \label{eq:Phi}
\Phi := \bigcup_{\{\vec{x}_i\}_{i=1}^{N_{\text{train}}}} 
\condset{\vphi^j}{J_j\in\mathcal{L}^{(k)}}_{k=1}^K
\subset\mathcal{F}
\end{equation}
where the feature space $\mathcal{F}$ is described in \cref{tbl:jssp:feat}, and 
are based on job- and machine-attributes which are widespread in practice.

\input{pseudocode/constructJSP}

%\subsection{Labelling schedules w.r.t. optimal decisions}
It is easy to see that the sequence of task assignments is by no means unique. 
Inspecting a partial schedule further along in the dispatching process such as 
in \cref{fig:jssp:example}, then let's say $J_1$ would be dispatched next, and 
in the next iteration $J_2$. Now this sequence would yield the same schedule as 
if $J_2$ would have been dispatched first and then $J_1$ in the next iteration, 
i.e., these are non-conflicting jobs.  In this particular instance, one cannot 
infer that choosing $J_1$ is better and $J_2$ is worse (or vice versa) since
they can both yield the same solution. Furthermore, 
there may be multiple optimal solutions to the same 
problem instance. Hence not only is the sequence representation `flawed' in the 
sense that slight permutations on the sequence are in fact equivalent w.r.t. 
the end-result, but very varying permutations on the dispatching sequence 
(although given the same partial initial sequence) can result in very different 
complete schedules but can still achieve the same makespan. 
%Care must be taken in this case that neither resulting features are labelled 
%as undesirable. Only the resulting features from a dispatch resulting in a 
%suboptimal solution should be labelled undesirable.

The redundancy in building optimal solutions using dispatching rules means that 
many different dispatches may yield an optimal solution to the problem instance.
Let's formalise the probability of optimality (or stepwise 
classification accuracy) for a given policy $\pi$, is defined as, 
\begin{equation} \label{eq:tracc:opt}
\xi_{\pi} := \mathbb{E}\left\{\pi_{\star} = \pi \;\mid\; \pi_{\star} \right\}
\end{equation}
that is to say the mean likelihood of our policy $\pi$ being equivalent to the 
expert policy $\pi_\star$.
The probability that a job chosen by a SDR yields an optimal makespan on a 
step-by-step basis, i.e., $\xi_{\langle \text{SDR} \rangle}$, is depicted in 
\cref{fig:opt:SDR:xi}. These probabilities
vary quite a bit between the different problem instances distributions studied. 
From \cref{fig:opt:SDR:xi} one observed that $\xi_{\text{MWR}}$ has a higher 
probability than random guessing, in choosing a dispatch which may result in an 
optimal schedule. This is especially true towards the end of the schedule 
building process. Similarly, the $\xi_{\text{LWR}}$ chooses dispatches 
resulting in optimal schedules with a higher probability. This would appear to 
be support the idea that the higher the probability of dispatching jobs that 
may lead to an optimal schedule, the better the SDRs performance, as 
illustrated by \cref{fig:boxplot:SDR}. However, there is a counter example, 
$\xi_{\text{SPT}}$ has a higher probability than random dispatching of 
selecting a jobs that may lead to an optimal solution. Nevertheless, the random 
dispatching performs better than SPT on problem instances \jrnd{10}{10} and 
\jrndn{10}{10}. 


\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/{trdat_prob_moveIsOptimal_10x10_SDR}.pdf}
  \caption{Probability of SDR being optimal, ${\xi}_{\langle\text{SDR}\rangle}$}
  \label{fig:opt:SDR:xi}
\end{figure}

Looking at \cref{fig:opt:SDR:xi}, then \jrnd{10}{10}  has a relatively high 
probability ($70\%$ and above) of choosing an optimal job at random. 
However, it is imperative to keep making optimal decisions, because once off 
the optimal track the consequences are unknown. 
To demonstrate this \cref{fig:case} depicts mean worst and best case scenario 
of the resulting \namerho, once off the optimal track. Note, that 
this is given that one makes \emph{one} non-optimal dispatch. 
Generally, there will be more, and then the compound effects of making 
suboptimal decisions cumulate. 

It is interesting to observe that for \jrnd{10}{10} and \jrndn{10}{10} making suboptimal decisions later impacts on the resulting makespan more than doing a mistake early. 
The opposite seems to be the case for \frnd{10}{10}. 
In this case it is imperative to make good decisions right from the start. This is 
due to the major structural differences between JSP and FSP, namely the latter 
having a homogeneous machine ordering, constricting the solution immensely. 

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/{stepwise_10x10_OPT_casescenario}.pdf}
  \caption{Mean deviation from optimality, $\rho$, (\%), for best (lower 
    bound) and worst (upper bound) case scenario of choosing suboptimal 
    dispatch for \jrnd{10}{10}, \jrndn{10}{10} and \frnd{10}{10}}
  \label{fig:case}
\end{figure}

\subsection{Blended \dr s}\label{sec:opt:bdr}
A naive approach to create a simple blended \dr~(BDR) would be to 
switch between SDRs at a predetermined time.
Observing again \cref{fig:opt:SDR:xi}, a presumably good BDR for 
\jrnd{10}{10} would be to start with $\xi_{\text{SPT}}$ and then switch over to 
$\xi_{\text{MWR}}$ at around time step $k=40$, where the SDRs change places in 
outperforming one another. 
A box-plot for $\rho$ for the BDR compared with MWR and SPT is depicted in 
\cref{fig:boxplot:BDR} and its main statistics are reported in 
\cref{tbl:BDR:stats}. 
This simple swap between SDRs does outperform the SPT, yet 
doesn't manage to gain the performance edge of MWR. Using SPT downgrades the performance of MWR.

A reason for this lack of performance of our proposed BDR is perhaps that by 
starting out with SPT in the beginning, it sets up the schedules in such a way 
that it's quite greedy and only takes into consideration jobs with shortest 
immediate processing times. Now, even though it is possible to find optimal 
schedules from this scenario, as \cref{fig:opt:SDR:xi} show, the inherent 
structure that's already taking place, might make it hard to come across by 
simple methods. Therefore it's by no means guaranteed that by simply swapping 
over to MWR will handle that situation which applying SPT has already created. 
\Cref{fig:boxplot:BDR} does however show, that by applying MWR instead of SPT 
in the latter stages, does help the schedule to be more compact w.r.t. SPT. 
However, in the case of \jrnd{10}{10}  and \jrndn{10}{10}  the fact remains 
that the schedules have diverged too far from what MWR would have been able to 
achieve on its own. 

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/j_rnd/{boxplotRho_BDR_10x10}.pdf}
  \caption{Box-plot for deviation from optimality, $\rho$, (\%) for BDR where 
    SPT is applied for the first 10\%, 15\% or 40\% of the dispatches, followed 
    by MWR}
  \label{fig:boxplot:BDR}
\end{figure}
\input{tables/stats.BDR.10x10.tex}

In \cref{fig:opt:SDR:xi} we inspected the stepwise optimality, given that we 
were on the optimal trajectory. Since we're bound to make mistakes at some 
points, it's interesting to see how that stepwise optimality evolves for its 
intended trajectory, thereby updating \cref{eq:tracc:opt} to
\begin{equation} \label{eq:tracc:track}
\hat{\xi}_{\pi} := \mathbb{E}\left\{\pi_{\star} = \pi \;\mid\; \pi \right\}
\end{equation}
\Cref{fig:opt:SDR:xihat} shows the log likelihood for $\hat{\xi}_{\langle 
\text{SDR} \rangle}$ using \jrnd{10}{10}.
There we can see that even though $\hat{\xi}_{\text{SPT}}$ is generally more 
likely to find optimal dispatches in the initial steps, then shortly after 
$k=15$, $\hat{\xi}_{\text{MWR}}$ becomes a 
contender again. This could explain why our BDR switch at $k=40$ from 
\cref{fig:boxplot:BDR} was unsuccessful. However, changing to MWR at $k=10$ or 
$k=15$ is not statically significant from MWR (boost in mean $\rho$ is at most 
0.5\%). But as pointed out for \cref{fig:case}, it's not so fatal to make bad 
moves in the very first dispatches for \jrnd{10}{10}, hence little gain with 
improved classification accuracy in that region.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/j_rnd/{trdat_prob_moveIsOptimal_10x10_SDR_xihat}.pdf}
  \caption{Log likelihood of SDR being optimal for \jrnd{10}{10}, when 
  following its corresponding SDR trajectory, i.e.,   
  $\log\left(\hat{\xi}_{\langle\text{SDR}\rangle}\right)$}
  \label{fig:opt:SDR:xihat}
\end{figure}


\section{Learning Composite Dispatching Rules}\label{ch:expr:CDR}
\Cref{sec:opt:bdr} demonstrated there is definitely something to be gained by
trying out different combinations of DRs, it's just non-trivial how to go about 
it, and motivates how it's best to go about learning such interaction, which 
will be addressed in this \lcnamecref{ch:expr:CDR}.

Learning models considered in this study are based on ordinal regression in 
which the learning task is formulated as learning preferences. In the case of 
scheduling, learning which operations are preferred to others. Ordinal 
regression has been previously presented in \cite{Ru06:PPSN} and in 
\cite{InRu11a} for \JSP, and given here for completeness. 

\subsection{Preference Learning}
The optimum makespan is known for each problem instance. At each time step $k$, 
a number of feature pair are created. 
Let $\vphi^{o}\in\R^d$ denote the post-decision state when dispatching 
$J_o\in\mathcal{O}^{(k)}$ corresponds to an optimal schedule being built. 
All post-decisions states corresponding to suboptimal dispatches, 
$J_s\in\mathcal{S}^{(k)}$, are denoted by $\vphi^{s}\in\R^d$.
Note, \mbox{$\mathcal{O}^{(k)}\cup\mathcal{S}^{(k)}=\mathcal{L}^{(k)}$}, and 
\mbox{$\mathcal{O}^{(k)}\cap\mathcal{S}^{(k)}=\emptyset$}.

The approach taken here is to verify analytically, at each time step, by fixing 
the current temporal schedule as an initial state, whether it can indeed 
\emph{somehow} yield an optimal schedule by manipulating the remainder of the 
sequence. This also takes care of the scenario that having dispatched a job 
resulting in a different temporal makespan would have resulted in the same 
final makespan if another optimal dispatching sequence would have been chosen. 
That is to say the training data generation takes into consideration when there 
are multiple optimal solutions\footnote{
  There can be several optimal solutions available for each problem instance. 
  However, it is deemed sufficient to inspect only one optimal trajectory per 
  problem instance as there are $N_{\text{train}}=300$ independent instances 
  which gives the training data variety.} 
to the same problem instance. 

Let's label features from \cref{eq:Phi} that were considered optimal, 
\mbox{$\vpsi^{o}=\vphi^{o}-\vphi^{s}$}, and suboptimal, 
\mbox{$\vpsi^{s}=\vphi^{s}-\vphi^{o}$} by $y_o=+1$ and $y_s=-1$ respectively.  
Then, the preference learning problem is specified by a set of preference pairs,
\begin{equation}
\Psi = 
\left\{\left(\vpsi^o,+1\right),\left(\vpsi^s,-1\right)
\;\middle|\;\forall \left(J_o,J_s\right) \in \mathcal{O}^{(k)} \times 
\mathcal{S}^{(k)}\right\}_{k=1}^{K} \subset \Phi\times Y \label{eq:prefset}
\end{equation}
where $\Phi\subset \mathbb{R}^d$ is the training set of $d=\NrFeatLocal$ 
features (cf. \cref{tbl:jssp:feat}), $Y=\{+1,-1\}$ is the outcome space from 
job pairs, $J_o\in\mathcal{O}^{(k)}$ and $J_s\in\mathcal{S}^{(k)}$, for all 
dispatch steps $k$.

To summarise, each job is compared against another job of the job-list, 
$\mathcal{L}^{(k)}$, and if the makespan differs, i.e., $C_{\max}^{(s)}\gneq 
C_{\max}^{(o)}$, an optimal/suboptimal pair is created. 
However, if the makespans are identical the pair is omitted since they give the 
same optimal makespan. 
This way, only features from a dispatch resulting in a suboptimal solution is 
labelled undesirable.

Now let's consider the model space $\mathcal{H} = \{\pi(\cdot) : X \mapsto Y\}$ 
of mappings from solutions to ranks. Each such 
function $\pi$ induces an ordering $\succ$ on the solutions  by the following 
rule,
\begin{equation}\label{eq:linear}
\vchi^i \succ \vchi^j \quad \Leftrightarrow \quad \pi(\vchi^i) > 
\pi(\vchi^j)
\end{equation}
where the symbol $\succ$ denotes ``is preferred to.''  The function used to 
induce the preference is defined by a linear function in the feature space,
\begin{equation} 
\pi(\vchi^j)=\sum_{i=1}^d w_i\phi_i(\vchi^j)=\inner{\vec{w}}{\vphi(\vchi^j)}.
\end{equation}

Logistic regression learns the optimal parameters $\vec{w}^*\in\mathbb{R}^d$. 
For this study, L2-regularized logistic regression from the \textsc{liblinear} 
package \cite{liblinear} without bias is used to learn the preference set 
$\Psi$, defined by \cref{eq:prefset}.
Hence, for each job on the job-list, $J_j\in\mathcal{L}$, let 
$\vphi^j:=\vphi(\vchi^j)$ denote its corresponding  post-decision state. 
Then the job chosen to be dispatched, $J_{j^*}$, is the one corresponding to 
the highest preference estimate, i.e., \cref{eq:jstar} where $h(\cdot)$ is the 
classification model obtained by the preference set.

Preliminary experiments for creating step-by-step model was done in 
\cite{InRu11a} where an optimal trajectory was explored, i.e., at each dispatch 
some (random) optimal task is dispatched, resulting in local linear model for 
each dispatch; a total of $K$ linear models for solving $n\times m$ JSP. 
However, the experiments there showed that by fixing the weights to its mean 
value throughout the dispatching sequence, results remained satisfactory.
A more sophisticated way, would be to create a \emph{new} linear model, where 
the preference set, $\Psi$, is the union of the preference pairs across the 
$K$ dispatches, such as described in \cref{eq:prefset}. 
This would amount to a substantial preference set, and for $\Psi$ to be 
computationally feasible to learn, $\Psi$ has to be reduced. For this several 
ranking strategies were explored in \cite{InRu15a}, the results there showed 
that it's sufficient to use partial subsequent rankings, namely, combinations 
of $r_i$ and $r_{i+1}$ for $i\in\{1,\ldots,n'\}$, are added to the preference 
set, where $r_1>r_2>\ldots>r_{n'}$ ($n'\leq n$) are the rankings of the 
job-list, in such a manner that in the cases that there are more than one 
operation with the same ranking, only one of that rank is needed to be compared 
to the subsequent rank. 
Moreover, for this study, which deals with $10\times 10$ problem instances, 
the partial subsequent ranking becomes necessary, as full ranking is 
computationally infeasible due to its size. 

Defining the size of the preference set as $l=\abs{\Psi}$, then if  $l$ is too 
large re-sampling may be needed to be done in order for the ordinal regression 
to be computationally feasible. 

\subsection{Collecting training data}

The training data from \cite{InRu11a} was created from optimal solutions of 
randomly generated problem instances, i.e., traditional \emph{passive} 
imitation learning (IL). 
As \JSP\ is a sequential decision making process, errors are bound to emerge.  
Due to compound effect of making suboptimal dispatches, the model leads the 
schedule astray from learned state-spaces, resulting in the new input being 
foreign to the learned model. 

Alternatively, training data could be generated using suboptimal solution 
trajectories as well, as was done in \cite{InRu15a}, where the training data 
also incorporated following the trajectories obtained by applying successful 
SDRs from the literature. 
The reasoning behind it was that  they would be beneficial for learning, 
as they might help the model to escape from local minima once off the coveted 
optimal path. 
By simply adding training data obtained by following the trajectories of 
well-known SDRs, their aggregated training set yielded better models with lower 
\fullnamerho. 

Inspired by the work of \cite{RossB10,RossGB11}, the methodology of generating 
training data will now be such that it will iteratively improve upon the model, 
such that the state-spaces learned will be representative of the state-spaces 
the eventual model would likely encounter, known as DAgger for \emph{active} 
imitation learning.
Thereby, eliminating the ad-hoc nature of choosing trajectories to learn, by 
rather letting the model lead its own way in a self-perpetuating manner until 
it converges.

Furthermore, In order to boost training accuracy, two strategies were explored 
\begin{enumerate}[after={{}}, leftmargin=*,
  label={\textbf{Boost.\arabic*}}, ref={{Boost.\arabic*}}]
  \item \label{expr:boost:varylmax} increasing number of preferences used 
  in training (i.e. varying \mbox{$l_{\max} \leq \abs{\Psi}$}),
  \item \label{expr:boost:newdata} introducing more problem instances (denoted 
  EXT in experimental setting).
\end{enumerate}
Note, that in preliminary experiments for \ref{expr:boost:varylmax} showed no 
statistical significance in boost of performance. Hence, the default set-up 
will be, $l_{\max}=5 \cdot 10^5$, which is roughly the amount of features 
encountered from one pass of sampling a \mbox{$K$-stepped} trajectory using a 
fixed policy $\pi$ for the default $N_{\text{train}}=300$.

\section{Passive Imitation Learning}\label{sec:il:passive}
Using the terms from game-theory used in \cite{CesaBianchi06}, % chapter 2: 
then our problem is a basic version of the sequential prediction problem where 
the predictor (or forecaster), $\pi$, observes each element of a sequence 
$\vchi$ of jobs, where at each time step $k \in \{1,...,K\}$, before the 
$k$-th job of the sequence is revealed, the predictor guesses its value 
$\chi_k$ on the basis of the previous $k-1$ observations. 

\subsection{Prediction with Expert Advice}\label{sec:expertPolicy}
Let's assume we know the expert policy $\pi^\star$, which we can query what 
is the optimal choice of $\chi_k={j^*}$ at any given time step $k$. 
Now we can use \cref{eq:jstar} to back-propagate the relationship between 
post-decision states and $\hat{\pi}$ with preference learning via our collected 
feature set, denoted $\Phi^\text{OPT}$, i.e., we collect the features set 
corresponding following optimal tasks $J_{j^*}$ from $\pi^\star$ in 
\cref{pseudo:constructJSP}.
This baseline trajectory sampling for adding features to the feature set 
is a pure strategy where at each dispatch, an optimal task was originally 
introduced in \cite{InRu11a}.

By querying the expert policy, $\pi_\star$, the ranking of the job-list, 
$\mathcal{L}$, is determined such that,
\begin{equation}
r_1 \succ r_2 \succ \cdots \succ r_{n'} \quad (n' \leq n)
\end{equation}
implies $r_1$  is preferable to $r_2$, and $r_2$ is preferable to $r_3$, etc. 
In  our study, we know $r \propto C_{\max}^{\pi_\star}$, hence the optimal 
job-list is the following, 
\begin{equation}
\mathcal{O}=\condset{r_i}{r_i \propto \min_{J_j \in \mathcal{L}} 
  C_{\max}^{\pi_\star(\vchi^j)}}
\end{equation}
found by solving the current partial schedule to optimality using a 
commercial software package such as \cite{gurobi}. 

When $\abs{\mathcal{O}^{(k)}}>1$, there can be several trajectories worth 
exploring. However, only one is chosen at random. This is deemed sufficient as 
the number of problem instances, $N_{\text{train}}$, is relatively large.

\subsection{Follow the Perturbed Leader}\label{sec:perturbedLeader}
By allowing a predictor to randomise it's possible to achieve improved 
performance \cite{CesaBianchi06,Hannan57}, which is the inspiration for our new 
strategy, where we follow the Perturbed Leader, denoted OPT$\epsilon$. 
Its pseudo code is given in \cref{pseudo:perturbedLeader} and describes how the 
expert policy (i.e. optimal trajectory) from \cref{sec:expertPolicy} is subtly
``perturbed'' with $\epsilon=10\%$ likelihood, by choosing a job corresponding 
to the second best $C_{\max}$ instead of a optimal one with some small 
probability. 

\input{pseudocode/perturbedLeader}

\subsection{Results}\label{sec:pil:expr}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/{j_rnd}/{boxplot_passive_10x10}.pdf}
  \caption{Box plot for \jrnd{10}{10} \namerho, using either expert policy and  
  following perturbed leader.}\label{fig:passive:boxplot} 
\end{figure}

Results for \jrnd{10}{10} box-plot of \namerho, is given in 
\cref{fig:passive:boxplot} and main statistics are reported in 
\cref{tbl:IL:stats}. 
To address \ref{expr:boost:newdata}, the extended training set was simply 
obtained by iterating over more examples, $N^{\text{OPT}}_{\text{train, 
EXT}}=1000$. However, we see that the increased number of varied features 
dissuades the preference models to achieving a good performance w.r.t. $\rho$. 
It's preferable to use the default $N^{\text{OPT}}_{\text{train}}=300$ and 
allowing slightly perturbing the optimal trajectory, as done for  
$\Phi^{\text{OPT}\epsilon}$. Unfortunately, all this overhead has not managed 
to surpass MWR in performance. 

From \cite{InRu11a} we know that expert policy is a promising starting point.
However, that was for $6\times5$ dimensionality (i.e. $K=30$), which is a much 
simpler problem space. Notice that in \cref{fig:opt:SDR:xihat} there was 
virtually no chance for $\hat{\xi}_\pi$ of choosing a job resulting in optimal 
makespan after step $k=28$.

Since \jsp\ is a sequential prediction problem, all future observations are 
dependent on previous operations. 
Therefore, learning sampled states that correspond only to optimal or 
near-optimal schedules isn't of much use when the preference model has 
diverged too far. We know from \cref{sec:opt:bdr}, that good classification 
accuracy based on $\xi_\pi$ doesn't necessarily mean a low mean \namerho.
This is due to the learner's predictions affects future input observations 
during its execution, which violates the crucial i.i.d. assumptions of the  
learning approach, and ignoring this interaction leads to poor performance.
In fact, \cite{RossB10} proves, that assuming the preference model has a 
training error of $\epsilon$, then the total compound error (for all $K$ 
dispatches) the classifier induces itself grows quadratically, $\bigOh{\epsilon 
  K^2}$, for the entire schedule, rather than having linear loss, 
$\bigOh{\epsilon K}$, if it were i.i.d.


\section{Active Imitation Learning}\label{sec:il:active}

To amend performance from $\Phi^{\text{OPT}}$-based models, suboptimal 
state-spaces were explored in \cite{InRu15a} by inspecting the features from 
successful SDRs, $\Phi^{\langle\text{SDR}\rangle}$, by passively observing a 
full execution of following the task chosen by the corresponding SDR. 
This required some trial-and-error as the experiments showed that features 
obtained by SDR trajectories were not equally useful for learning.

To automate this process, inspiration from \emph{active} imitation learning 
presented in \cite{RossGB11} is sought, called \emph{Dataset Aggregation} 
(DAgger) method, which addresses a no-regret algorithm in an on-line learning 
setting. 
The novel meta-algorithm for IL learns a deterministic policy guaranteed to 
perform well under its induced distribution of states. 
The method is closely related to Follow-the-leader (cf. 
\cref{sec:perturbedLeader}), however, with a more sophisticated leverage to the 
expert policy. 
In short, it entails the model $\pi_i$ that queries an expert policy (same as 
in \cref{sec:expertPolicy}), $\pi_\star$, its trying to mimic, 
but also ensuring the learned model updates itself in an iterative fashion, 
until it converges. 
The benefit of this approach is that the states that are likely to occur in 
practice are also investigated and as such used to dissuade the model from 
making poor choices. In fact, the method queries the expert about the desired 
action at individual post-decision states which are both based on past queries, 
and the learner's interaction with the \emph{current} environment.

DAgger has been proven successful on a variety of benchmarks, such as
\begin{enumerate*}[label={{}}]
  \item the video games Super Tux Kart and Super Mario Bros. or
  handwriting recognition -- in all cases greatly improving traditional 
  supervised imitation learning approaches \cite{RossGB11}
  \item real-world applications, e.g. autonomous navigation for large unmanned 
  aerial vehicles \cite{Ross13}
\end{enumerate*}
To illustrate the effectiveness of DAgger, the Super Mario Bros. experiment 
gives a very simple and informative understanding of the benefits of the 
algorithm. In short, Super Mario Bros. is a platform game where the 
protagonist, Mario, must move across the stage without being hit by enemies or 
falling through gaps within a certain time limit. 
One of the reasons the supervised approaches failed, were due to Mario getting 
stuck up against an obstacle, instead of jumping over it. 
However, the expert would always jump over them at a greater distance 
beforehand, and therefore the learned controller would not know of these 
scenarios. 
With iterative methods, Mario would encounter these problematic situations and 
eventually learn how to get himself unstuck. 

\subsection{DAgger}
The policy of imitation learning at iteration $i>0$ is a mixed strategy given 
as follows, 
\begin{equation}\label{eq:il}
\pi_i = \beta_i\pi_\star + (1-\beta_i)\hat{\pi}_{i-1}
% i: ith iteration of imitation learning
% pi_star is expert policy (i.e. optimal)
% pi_i^hat: is pref model from prev. iteration
\end{equation}
where $\pi_\star$ is the expert policy and $\hat{\pi}_{i-1}$ is the learned 
model from the previous iteration. 
Note, for the initial iteration, $i=0$, a pure strategy of $\pi_\star$ is 
followed. Hence, $\hat{\pi}_0$ corresponds to the preference model from 
\cref{sec:expertPolicy} (i.e. $\Phi^{\text{IL}0}=\Phi^{\text{OPT}}$). 

\Cref{eq:il} shows that $\beta$ controls the probability distribution of 
querying the expert policy $\pi_\star$ instead of the previous imitation model, 
$\hat{\pi}_{i-1}$.  
The only requirement for $\{\beta_i\}_i^\infty$ according to \cite{RossGB11} is 
that $\limit{\frac{1}{T}\sum_{i=0}^T\beta_i}{T\to\infty}{0}$ to guarantee 
finding a policy $\hat{\pi}_i$ that achieves $\epsilon$ surrogate loss under 
its own state distribution limit.

\Cref{pseudo:activeIL} explains the pseudo code for how to collect 
partial training set, $\Phi^{\text{IL}i}$ for $i$-th iteration of active 
imitation learning.
Subsequently, the resulting preference model, $\hat{\pi}_i$, learns on the 
aggregated datasets from all previous iterations, namely,  
\begin{equation}\label{eq:DAgger}
\Phi^{\text{DA}i}=\bigcup_{i'=0}^{i}\Phi^{\text{IL}i'}
\end{equation}
and its update procedure is detailed in \cref{pseudo:DAgger}.

\input{pseudocode/imitationLearning}
\input{pseudocode/DAgger}

\subsection{Results}\label{sec:ail:expr}
Due to time constraints, only $T=3$ iterations will be inspected.
In addition, preliminary experiments showed that DAgger for \jsp\ is not 
sensitive to choice of $\beta_i$ in \cref{eq:il}. 
Hence, a simple parameter-free version of the DAgger algorithm, which often 
performs best in practice \cite{RossGB11}, is chosen. 
Namely, the mixed strategy for $\{\beta_i\}_{i=0}^T$ is \emph{unsupervised} 
with $\beta_i=I(i=0)$, where $I$ is the indicator 
function.\footnote{$\beta_0=1$ and $\beta_i=0,\forall i>0$.}

Regarding \ref{expr:boost:newdata} strategy, we know from 
\cref{sec:il:passive}, that adding new problem instances didn't boost 
performance for the expert policy (which is equivalent for the initial 
iteration of DAgger). 
Hence, for active IL, the extended set is now consisted of each iteration 
encountering $N_{\text{train}}$ \emph{new} problem instances. For a grand total 
of 
\begin{equation}
N^{\text{DA}i}_{\text{train, EXT}}=N_{\text{train}}\cdot (i+1) 
\end{equation}
problem instances explored for the aggregated extended training set used for 
the learning model at iteration $i$.
This way, we use the extended training data sparingly, as labelling for each 
problem instances is computationally intensive. As a result, the computational 
budget for DAgger is same regardless whether there are new problem instances 
used or not, i.e., 
$\abs{\Phi^{\text{DA}i}}\approx\abs{\Phi^{\text{DA}i}_{\text{EXT}}}$.

Results for \jrnd{10}{10} box-plot of \namerho, is given in 
\cref{fig:active:boxplot} and main statistics is reported in 
\cref{tbl:IL:stats}. As we can see DAgger is not fruitful when the same problem 
instances are continually used. This is due to the fact that there is not 
enough variance between $\Phi^{\text{IL}i}$, hence the aggregated feature set 
$\Phi^{\text{DA}i}$ is only slightly perturbed with each iterations. Which from 
\cref{sec:pil:expr} we saw wasn't a very successful modification for the expert 
policy. Although, it's noted that by introducing sub-optimal state spaces 
the preference model is not as drastically bad as the extended optimal policy, 
even though 
$\abs{\Phi^{\text{DA}i}}\approx\abs{\Phi^{\text{OPT}}_{\text{EXT}}}$.
However, when using new problem instances at each iterations, the feature set 
becomes varied enough that situations arise that can be learned to achieve a 
better represented classification problem which yields a lower mean \namerho.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/{j_rnd}/{boxplot_active_10x10}.pdf}
  \caption{Box plot for \jrnd{10}{10} \namerho, using DAgger for \JSP}
  \label{fig:active:boxplot} 
\end{figure}

\section{Summary of Imitation Learning}\label{sec:il:expr}

A summary of \jrnd{10}{10} best passive and active imitation learning models
w.r.t. \namerho, from 
\cref{sec:pil:expr,sec:ail:expr}, respectively, are illustrated in 
\cref{fig:all:boxplot}, and main statistics are given in 
\cref{tbl:IL:stats}. To summarise, the following trajectories are used
\begin{enumerate*}
  \item expert policy, trained on $\Phi^{\text{OPT}}$
  \item perturbed leader, trained on $\Phi^{\text{OPT}\epsilon}$
  \item imitation learning, trained on $\Phi^{\text{DA}i}_{\text{EXT}}$ for 
  iterations $i=\{1,..,3\}$ using extended training set
\end{enumerate*}
As a reference, the \sdr\ MWR is shown on the far right of 
\cref{fig:all:boxplot}.

At first we see that the perturbed leader ever so-slightly improves the mean 
for $\rho$, rather than using the baseline expert policy. 
However, active imitation learning is by far the best improvement. With each 
iteration of DAgger, the models improve with each iteration, even outperforming 
MWR after $T=2$ iterations.

Regarding \ref{expr:boost:newdata}, then it's not successful for the expert 
policy, as $\rho$ increases approximately 10\%. This could most likely be 
counter-acted by increasing $l_{\max}$ to reflect the 700 additional examples.
What is interesting though, is that \ref{expr:boost:newdata} is well suited for 
active imitation learning, using the same $l_{\max}$ as before. 
Note, the amount of problems used for $N^{\text{OPT}}_{\text{train, EXT}}$ is 
equivalent to $T=2\tfrac{1}{3}$ iterations of extended DAgger.
The \emph{new} varied data gives the aggregated feature set more information 
of what is important to learn in subsequent iterations, as those new states are 
more likely to be encountered `in practice' rather than `in theory.' Not only 
does the active imitation learning converge faster, it also consistently 
improves with each iterations.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/{j_rnd}/{boxplot_summary_10x10}.pdf}
  \caption{Box plot for \jrnd{10}{10} \namerho, using either expert policy, 
  DAgger or following perturbed leader strategies. 
  MWR shown on far right for reference.}\label{fig:all:boxplot} 
\end{figure}

\input{tables/stats.IL.10x10.tex}

\section{Discussion and conclusions}\label{sec:con}
Current literature still hold \sdr s in high regard, 
as they are simple to implement and quite efficient. 
However, they are generally taken for granted as there is clear lack of 
investigation of \emph{how} these \dr s actually work, and what 
makes them so successful (or in some cases unsuccessful)? 
For instance, of the four SDRs this study focuses on, why does MWR outperform 
so significantly for \jsp\, yet completely fail for \fsp? 
MWR seems to be able to adapt to varying distributions of processing times, 
however, manipulating the machine ordering causes MWR to break down. 
By inspecting optimal schedules, and meticulously researching what's going on, 
every step of the way of the dispatching sequence, some light is shed where 
these SDRs vary w.r.t. the problem space at hand. 
Once these simple rules are understood, then it's feasible to extrapolate the 
knowledge gained and create new \cdr s that are likely to be 
successful. 

Creating new \dr s is by no means trivial. For \jsp\ there is 
the hidden interaction between processing times and machine ordering that's 
hard to measure.
Due to this artefact, feature selection is of paramount importance, and then it 
becomes the case of not having too many features, as they are likely to hinder 
generalisation due to over-fitting in training. However, the features need to 
be explanatory enough to maintain predictive ability. 
%For this reason \cref{ch:expr:CDR} was limited to up to three active features, 
%as the full feature set was clearly suboptimal w.r.t. the SDRs used as a 
%benchmark. 
%By using features based on the SDRs, along with some additional local features 
%describing the current schedule, it was possible to `discover' the SDRs when 
%given only one active feature. %Although there is not much to be gained by 
%these models, they at least are a sanity check the learning models are on the 
%right track. 
%Furthermore, by adding on additional features, a boost in performance was 
%gained, resulting in a \cdr\ that outperformed all of the 
%SDR baseline. 

When training the learning model, it's not sufficient to only optimise w.r.t. 
highest mean validation accuracy. As there is a trade-off between making the 
over-all best decisions versus making the right decision on crucial time points 
in the scheduling process, as \cref{fig:case} clearly illustrated. 
%It is for this reason, traditional feature selection such 
%as add1 and drop1 were unsuccessful in preliminary experiments, and thus 
%resorting to having to exhaustively search all feature combinations.
This also opens of the question of how should validation accuracy be measured? 
Since the model is based on learning preferences, both based on optimal versus 
suboptimal, and then varying degrees of sub-optimality. As we are only looking 
at the ranks in a black and white fashion, such that the makespans need to be 
strictly greater to belong to a higher rank, then it can be argued that some 
ranks should be grouped together if their makespans are sufficiently close. 
This would simplify the training set, making it (presumably) less of 
contradictions and more appropriate for linear learning. Or simply the 
validation accuracy could be weighted w.r.t. the  difference in 
makespan.
During the dispatching process, there are some pivotal times which need to be 
especially taken care off. \Cref{fig:case} showed how making suboptimal 
decisions were more of a factor during the later stages, whereas for flow-shop 
the case was exact opposite. 
%\todo[inline]{Could discuss new sampling strategies, e.g., proportional to
%  best/worst case, optimality, etc. -- have done some experiments, but not
%  clear what strategy is best, so only equal probability reported}
Experiments in \cref{sec:pil:expr} clearly showed that following the expert 
policy is not without its faults. There are many obstacles to consider to 
improve the model. 
For instance, their experiments $\Psi$ to size $l$ with equal 
probability. But inspecting the effects of making suboptimal choices varies as 
a function of times steps, perhaps its stepwise bias should rather be done 
proportional to the mean cumulative loss to a particular time step? 
However, it's non-trivial to go about that. Preliminary experiments on sampling 
measures based on \cref{fig:boxplot:SDR} and \cref{fig:case} didn't show any 
performance boost in doing so.

Despite the abundance of information gathered by following an optimal 
trajectory, the knowledge obtained is not enough by itself. Since the learning 
model isn't perfect, it is bound to make a mistake eventually. When it does, 
the model is in uncharted  territory as there is not certainty the samples 
already collected are able to explain the current situation. For this we 
propose investigating features from suboptimal trajectories as well, since the 
future observations depend on previous predictions. 
A straight forward approach would be to inspect the trajectories of promising 
SDRs or CDRs. However, more information is gained when applying active 
imitation learning inspired by work of \cite{RossB10,RossGB11}, such that the 
learned policy following an optimal trajectory is used to collect training 
data, and the learned model is updated. 
This can be done over several iterations, with the benefit being, that the 
states that are likely to occur in practice are investigated, and as such used 
to dissuade the model from making poor choices. Alas, this comes at great 
computational cost due to the substantial amounts of states that need to be 
optimised for their correct labelling. Making it only practical for \jsp\ of 
a considerable lower dimension. 

Maximum Mean Discrepancy (MMD) imitation learning by \cite{Kim13} is an 
iterative algorithm similar to DAgger. 
However, the expert policy is only queried when needed in order to reduce 
computational cost. 
This occurs when a metric of a new state is sufficiently large enough from a 
previously queried states (to ensure diversity of learned optimal states). 
Moreover, in DAgger all data samples are equally important, irrespective of its 
iteration, which can require great number of iterations to learn how to recover 
from the mistakes of earlier policies. To address the naivety of the data 
aggregation, MMD suggests only aggregating a new data point 
if it is sufficiently different to previously gathered states, \emph{and} if 
the current policy has made a mistake. 
Additionally, there are multiple policies, each specializing in a particular 
region of the state space where previous policies made mistakes.
Although MMD has better empirical performance (based on robot applications), it 
requires defining metrics, which in the case of \jsp\ is non-trivial (cf. 
\cite{InRu12}), and fine-tuning thresholds etc., whereas DAgger can be 
straightforwardly implemented, parameter-free and obtains competitive results, 
although with some computational overhead due to excess expert queries. 

Main drawback of DAgger is that it quite aggressively quires the expert, making 
it impractical for some problems, especially if it involves human experts. 
To confront that, \cite{Judah12} introduce Reduction-based Active Imitation 
Learning (RAIL), which involves a dynamic approach similar to DAgger, but more 
emphasis is used to minimise the expert's labelling effort.
In fact, it's possible to circumvent querying the expert altogether and still 
have reasonable performance. By applying Locally Optimal Learning to Search 
(LOLS) \cite{ChangKADL15} it is possible to use imitation learning (similar to 
DAgger framework) when the reference policy is poor (i.e. $\pi_\star$ in 
\cref{eq:il} is suboptimal), 
although it's noted that the quality (w.r.t near-optimality) of reference 
policy is in accordance to its performance, as is to be expected. 

Although this study has been structured around the \jsp\ scheduling problem, 
it is easily extended to other types of deterministic optimisation problems 
that involve sequential decision making. 
The framework presented here collects snap-shots of the state space by 
following an optimal trajectory, and verifying the resulting optimal makespan 
from each possible state. 
From which the stepwise optimality of individual features can be inspected, 
which could for instance justify omittance in feature selection. 
Moreover, by looking at the best and worst case scenario of suboptimal 
dispatches, it is possible to pinpoint vulnerable times in the scheduling 
process. 

\bibliographystyle{spmpsci} 
\bibliography{references}  

\end{document}